{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CSV to Columnar Format Conversion\n",
        "\n",
        "This notebook demonstrates how to convert CSV files to Apache Parquet and Feather formats using vroom-csv.\n",
        "These columnar formats offer significant benefits:\n",
        "\n",
        "- **Parquet**: Highly compressed, optimized for storage and analytics\n",
        "- **Feather**: Fast read/write, ideal for intermediate files and interprocess communication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import vroom_csv\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import pyarrow.feather as feather\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# Create sample CSV data\n",
        "csv_content = \"\"\"id,name,age,salary,department,active\n",
        "1,Alice,30,75000.50,Engineering,true\n",
        "2,Bob,25,82000.00,Marketing,false\n",
        "3,Charlie,35,68000.25,Engineering,true\n",
        "4,Diana,28,91000.75,Sales,true\n",
        "5,Eve,32,85000.00,Marketing,false\n",
        "6,Frank,29,77500.25,Engineering,true\n",
        "7,Grace,31,88000.00,Sales,true\n",
        "8,Henry,27,72000.50,Marketing,false\n",
        "\"\"\"\n",
        "\n",
        "temp_dir = tempfile.mkdtemp()\n",
        "csv_path = os.path.join(temp_dir, \"employees.csv\")\n",
        "\n",
        "with open(csv_path, \"w\") as f:\n",
        "    f.write(csv_content)\n",
        "\n",
        "print(f\"Created CSV at: {csv_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read CSV with vroom-csv\n",
        "\n",
        "First, let's read the CSV file using vroom-csv's high-performance parser with automatic type inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read CSV with type inference\n",
        "table = vroom_csv.read_csv(csv_path)\n",
        "\n",
        "# Convert to PyArrow Table (zero-copy)\n",
        "arrow_table = pa.table(table)\n",
        "\n",
        "print(\"Schema (inferred types):\")\n",
        "print(arrow_table.schema)\n",
        "print(f\"\\nRows: {arrow_table.num_rows}\")\n",
        "print(f\"Columns: {arrow_table.num_columns}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert to Parquet\n",
        "\n",
        "Parquet is ideal for:\n",
        "- Long-term storage\n",
        "- Analytics workloads (e.g., Spark, BigQuery)\n",
        "- Efficient column pruning and predicate pushdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "parquet_path = os.path.join(temp_dir, \"employees.parquet\")\n",
        "\n",
        "# Write to Parquet with Snappy compression (default)\n",
        "pq.write_table(arrow_table, parquet_path, compression='snappy')\n",
        "\n",
        "# Compare file sizes\n",
        "csv_size = os.path.getsize(csv_path)\n",
        "parquet_size = os.path.getsize(parquet_path)\n",
        "\n",
        "print(f\"CSV size: {csv_size:,} bytes\")\n",
        "print(f\"Parquet size: {parquet_size:,} bytes\")\n",
        "print(f\"Compression ratio: {csv_size / parquet_size:.2f}x\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read back the Parquet file to verify\n",
        "parquet_table = pq.read_table(parquet_path)\n",
        "\n",
        "print(\"Data from Parquet:\")\n",
        "print(parquet_table.to_pandas())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parquet Compression Options\n",
        "\n",
        "Parquet supports several compression codecs with different tradeoffs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compression_options = ['none', 'snappy', 'gzip', 'zstd', 'lz4']\n",
        "\n",
        "print(\"Compression comparison:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for codec in compression_options:\n",
        "    path = os.path.join(temp_dir, f\"employees_{codec}.parquet\")\n",
        "    pq.write_table(arrow_table, path, compression=codec)\n",
        "    size = os.path.getsize(path)\n",
        "    print(f\"{codec:12} : {size:6,} bytes ({csv_size / size:5.2f}x compression)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert to Feather\n",
        "\n",
        "Feather (Arrow IPC format) is ideal for:\n",
        "- Fast temporary files\n",
        "- Interprocess communication\n",
        "- Intermediate results in data pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feather_path = os.path.join(temp_dir, \"employees.feather\")\n",
        "\n",
        "# Write to Feather format\n",
        "feather.write_feather(arrow_table, feather_path)\n",
        "\n",
        "feather_size = os.path.getsize(feather_path)\n",
        "print(f\"Feather size: {feather_size:,} bytes\")\n",
        "print(f\"\\nNote: Feather prioritizes read/write speed over compression\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read back the Feather file\n",
        "feather_table = feather.read_table(feather_path)\n",
        "\n",
        "print(\"Data from Feather:\")\n",
        "print(feather_table.to_pandas())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Working with Large Files\n",
        "\n",
        "For large CSV files, vroom-csv's SIMD-accelerated parsing combined with columnar output provides significant performance benefits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "\n",
        "# Generate a larger test file\n",
        "large_csv_path = os.path.join(temp_dir, \"large_data.csv\")\n",
        "n_rows = 100000\n",
        "\n",
        "with open(large_csv_path, \"w\") as f:\n",
        "    f.write(\"id,value1,value2,value3,category\\n\")\n",
        "    for i in range(n_rows):\n",
        "        f.write(f\"{i},{random.random():.4f},{random.randint(0, 1000)},{random.random() * 100:.2f},cat_{i % 10}\\n\")\n",
        "\n",
        "print(f\"Generated {n_rows:,} rows\")\n",
        "print(f\"CSV size: {os.path.getsize(large_csv_path):,} bytes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time CSV parsing and conversion\n",
        "start = time.time()\n",
        "table = vroom_csv.read_csv(large_csv_path)\n",
        "arrow_table = pa.table(table)\n",
        "parse_time = time.time() - start\n",
        "\n",
        "print(f\"CSV parse time: {parse_time:.3f}s\")\n",
        "\n",
        "# Time Parquet write\n",
        "parquet_path = os.path.join(temp_dir, \"large_data.parquet\")\n",
        "start = time.time()\n",
        "pq.write_table(arrow_table, parquet_path, compression='snappy')\n",
        "write_time = time.time() - start\n",
        "\n",
        "print(f\"Parquet write time: {write_time:.3f}s\")\n",
        "print(f\"Total CSV -> Parquet: {parse_time + write_time:.3f}s\")\n",
        "\n",
        "# Compare sizes\n",
        "csv_size = os.path.getsize(large_csv_path)\n",
        "parquet_size = os.path.getsize(parquet_path)\n",
        "print(f\"\\nCompression: {csv_size:,} -> {parquet_size:,} bytes ({csv_size / parquet_size:.2f}x)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Polars for Conversion\n",
        "\n",
        "Polars provides built-in support for reading vroom-csv tables and writing to Parquet/Feather."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "\n",
        "# Read with vroom-csv, convert with Polars\n",
        "table = vroom_csv.read_csv(csv_path)\n",
        "df = pl.from_arrow(table)\n",
        "\n",
        "# Write to Parquet with Polars\n",
        "polars_parquet_path = os.path.join(temp_dir, \"polars_output.parquet\")\n",
        "df.write_parquet(polars_parquet_path, compression='zstd')\n",
        "\n",
        "print(f\"Written {os.path.getsize(polars_parquet_path):,} bytes via Polars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.rmtree(temp_dir)\n",
        "print(\"Cleaned up temporary files.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
