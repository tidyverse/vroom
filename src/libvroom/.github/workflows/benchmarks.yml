name: Benchmarks

on:
  # Run on tagged releases
  push:
    tags:
      - 'v*'
  # Allow manual triggering with optional comparison
  workflow_dispatch:
    inputs:
      compare_ref:
        description: 'Git ref to compare against (tag, branch, or commit SHA)'
        required: false
        default: ''
      benchmark_filter:
        description: 'Benchmark filter pattern (empty for all)'
        required: false
        default: ''

permissions:
  contents: read
  # Required for uploading artifacts
  actions: write

jobs:
  benchmark:
    name: Run Benchmarks (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for tag comparisons

    - name: Install dependencies (Ubuntu)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        sudo apt-get install -y cmake build-essential ccache python3-pip jq
        pip3 install --break-system-packages pandas matplotlib || pip3 install pandas matplotlib

    - name: Install dependencies (macOS)
      if: runner.os == 'macOS'
      run: |
        brew install cmake ccache jq
        pip3 install pandas matplotlib --break-system-packages || pip3 install pandas matplotlib

    - name: Cache ccache
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/ccache
          ~/Library/Caches/ccache
        key: ccache-benchmark-${{ runner.os }}-${{ github.sha }}
        restore-keys: |
          ccache-benchmark-${{ runner.os }}-

    - name: Configure ccache
      run: |
        ccache --set-config=max_size=500M
        ccache --set-config=compression=true
        ccache -z

    - name: Get system information
      id: sysinfo
      run: |
        echo "=== System Information ==="
        echo "Date: $(date)"
        echo "Host: $(hostname)"
        if [ "$RUNNER_OS" == "Linux" ]; then
          echo "CPU: $(grep 'model name' /proc/cpuinfo | head -1 | cut -d':' -f2 | xargs)"
          echo "Cores: $(nproc)"
          echo "Memory: $(free -h | grep '^Mem:' | awk '{print $2}')"
          CPU_INFO=$(grep 'model name' /proc/cpuinfo | head -1 | cut -d':' -f2 | xargs)
        else
          echo "CPU: $(sysctl -n machdep.cpu.brand_string)"
          echo "Cores: $(sysctl -n hw.ncpu)"
          CPU_INFO=$(sysctl -n machdep.cpu.brand_string)
        fi
        echo "cpu_info=$CPU_INFO" >> $GITHUB_OUTPUT

    - name: Configure CMake (Release)
      run: |
        cmake -B build -DCMAKE_BUILD_TYPE=Release -DBUILD_BENCHMARKS=ON

    - name: Build benchmark executable
      run: |
        cmake --build build --target libvroom_benchmark -j$(nproc 2>/dev/null || sysctl -n hw.ncpu)

    - name: Show ccache stats
      run: |
        ccache -s

    - name: Run benchmarks
      id: run_benchmarks
      timeout-minutes: 60
      run: |
        # Create output directory
        mkdir -p benchmark_output

        # Determine benchmark filter (validated and quoted to prevent injection)
        FILTER='${{ github.event.inputs.benchmark_filter }}'
        FILTER_ARG=""
        if [ -n "$FILTER" ]; then
          # Validate filter contains only safe characters (alphanumeric, _, -, /, .)
          if ! echo "$FILTER" | grep -qE '^[a-zA-Z0-9_./\-*]+$'; then
            echo "Error: Invalid benchmark filter pattern. Only alphanumeric, underscore, dash, dot, slash, and asterisk are allowed."
            exit 1
          fi
          FILTER_ARG="--benchmark_filter=$FILTER"
        fi

        # Get current ref for naming
        if [ -n "$GITHUB_REF_NAME" ]; then
          REF_NAME="${GITHUB_REF_NAME//\//-}"
        else
          REF_NAME=$(git rev-parse --short HEAD)
        fi

        TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
        RESULTS_FILE="benchmark_output/benchmark_${REF_NAME}_${RUNNER_OS}_${TIMESTAMP}.json"

        echo "Running benchmarks..."
        echo "Results will be saved to: $RESULTS_FILE"

        # Run benchmarks with JSON output
        ./build/libvroom_benchmark \
          --benchmark_format=json \
          --benchmark_out="$RESULTS_FILE" \
          --benchmark_repetitions=3 \
          --benchmark_report_aggregates_only=true \
          $FILTER_ARG

        # Store results file path for later steps
        echo "results_file=$RESULTS_FILE" >> $GITHUB_OUTPUT
        echo "ref_name=$REF_NAME" >> $GITHUB_OUTPUT

        # Print summary
        if [ -f "$RESULTS_FILE" ]; then
          echo "=== Benchmark Summary ==="
          BENCHMARK_COUNT=$(jq '.benchmarks | length' "$RESULTS_FILE" 2>/dev/null || echo "0")
          echo "Benchmarks completed: $BENCHMARK_COUNT"
        fi

    - name: Generate benchmark report
      if: success()
      run: |
        RESULTS_FILE="${{ steps.run_benchmarks.outputs.results_file }}"
        REF_NAME="${{ steps.run_benchmarks.outputs.ref_name }}"

        if [ -f "$RESULTS_FILE" ]; then
          # Generate markdown report
          REPORT_FILE="benchmark_output/benchmark_report_${REF_NAME}_${RUNNER_OS}.md"

          if [ -f "benchmark/report_generator.py" ]; then
            python3 benchmark/report_generator.py "$RESULTS_FILE" --output "$REPORT_FILE"
          fi

          # Create a summary for GitHub Actions
          echo "## Benchmark Results (${{ runner.os }})" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Reference:** $REF_NAME" >> $GITHUB_STEP_SUMMARY
          echo "**CPU:** ${{ steps.sysinfo.outputs.cpu_info }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Extract top 10 benchmarks by throughput
          echo "### Top Benchmarks by Throughput" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Benchmark | Time (ms) | Throughput |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|-----------|------------|" >> $GITHUB_STEP_SUMMARY

          jq -r '.benchmarks[] | select(.aggregate_name == "mean") | "\(.name) | \((.real_time / 1000000) | floor) | \(.bytes_per_second // 0 | . / 1073741824 | . * 100 | floor / 100) GB/s"' "$RESULTS_FILE" 2>/dev/null | head -10 >> $GITHUB_STEP_SUMMARY || echo "| (no data) | - | - |" >> $GITHUB_STEP_SUMMARY
        fi

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ runner.os }}-${{ steps.run_benchmarks.outputs.ref_name }}
        path: benchmark_output/
        retention-days: 90

  # Compare benchmarks when triggered manually with a comparison ref
  compare:
    name: Compare Benchmarks
    needs: benchmark
    if: github.event.inputs.compare_ref != ''
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y cmake build-essential python3-pip jq
        pip3 install --break-system-packages pandas matplotlib || pip3 install pandas matplotlib

    - name: Build benchmark for comparison ref
      run: |
        # Validate and checkout comparison ref
        COMPARE_REF='${{ github.event.inputs.compare_ref }}'
        # Validate ref contains only safe characters (alphanumeric, _, -, /, .)
        if ! echo "$COMPARE_REF" | grep -qE '^[a-zA-Z0-9_./\-]+$'; then
          echo "Error: Invalid compare_ref. Only alphanumeric, underscore, dash, dot, and slash are allowed."
          exit 1
        fi
        git checkout "$COMPARE_REF"

        # Build
        cmake -B build-compare -DCMAKE_BUILD_TYPE=Release -DBUILD_BENCHMARKS=ON
        cmake --build build-compare --target libvroom_benchmark -j$(nproc)

        # Run benchmarks
        mkdir -p comparison_output
        FILTER='${{ github.event.inputs.benchmark_filter }}'
        FILTER_ARG=""
        if [ -n "$FILTER" ]; then
          # Validate filter contains only safe characters (alphanumeric, _, -, /, .)
          if ! echo "$FILTER" | grep -qE '^[a-zA-Z0-9_./\-*]+$'; then
            echo "Error: Invalid benchmark filter pattern. Only alphanumeric, underscore, dash, dot, slash, and asterisk are allowed."
            exit 1
          fi
          FILTER_ARG="--benchmark_filter=$FILTER"
        fi

        ./build-compare/libvroom_benchmark \
          --benchmark_format=json \
          --benchmark_out="comparison_output/baseline.json" \
          --benchmark_repetitions=3 \
          --benchmark_report_aggregates_only=true \
          $FILTER_ARG

    - name: Download current benchmark results
      uses: actions/download-artifact@v4
      with:
        name: benchmark-results-Linux-${{ github.ref_name }}
        path: current_output/

    - name: Compare results
      run: |
        CURRENT_FILE=$(ls current_output/*.json | head -1)
        BASELINE_FILE="comparison_output/baseline.json"

        if [ -f "$CURRENT_FILE" ] && [ -f "$BASELINE_FILE" ]; then
          echo "## Benchmark Comparison" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Current:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Baseline:** ${{ github.event.inputs.compare_ref }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Generate comparison using report_generator.py if available
          if [ -f "benchmark/report_generator.py" ]; then
            python3 benchmark/report_generator.py "$CURRENT_FILE" \
              --baseline "$BASELINE_FILE" \
              --output comparison_output/comparison_report.md \
              --regression-threshold 10.0

            if [ -f "comparison_output/comparison_report.md" ]; then
              cat comparison_output/comparison_report.md >> $GITHUB_STEP_SUMMARY
            fi
          else
            # Basic comparison using jq
            echo "### Performance Comparison" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "| Benchmark | Current (ms) | Baseline (ms) | Change |" >> $GITHUB_STEP_SUMMARY
            echo "|-----------|--------------|---------------|--------|" >> $GITHUB_STEP_SUMMARY

            # Compare matching benchmarks
            jq -r '.benchmarks[] | select(.aggregate_name == "mean") | "\(.name)|\(.real_time)"' "$CURRENT_FILE" > /tmp/current.txt
            jq -r '.benchmarks[] | select(.aggregate_name == "mean") | "\(.name)|\(.real_time)"' "$BASELINE_FILE" > /tmp/baseline.txt

            while IFS='|' read -r name current_time; do
              baseline_time=$(grep "^$name|" /tmp/baseline.txt | cut -d'|' -f2)
              if [ -n "$baseline_time" ]; then
                current_ms=$(echo "scale=2; $current_time / 1000000" | bc -l)
                baseline_ms=$(echo "scale=2; $baseline_time / 1000000" | bc -l)
                change=$(echo "scale=1; (($current_time - $baseline_time) / $baseline_time) * 100" | bc -l)
                echo "| $name | $current_ms | $baseline_ms | ${change}% |" >> $GITHUB_STEP_SUMMARY
              fi
            done < /tmp/current.txt
          fi
        fi

    - name: Upload comparison results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-comparison
        path: comparison_output/
        retention-days: 90

  # Store benchmark results for historical tracking
  store-results:
    name: Store Results
    needs: benchmark
    if: startsWith(github.ref, 'refs/tags/')
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        ref: main

    - name: Download all benchmark artifacts
      uses: actions/download-artifact@v4
      with:
        path: downloaded_artifacts/

    - name: Store results in benchmark_results directory
      run: |
        mkdir -p benchmark_results/releases

        # Copy all benchmark results
        for artifact_dir in downloaded_artifacts/benchmark-results-*/; do
          if [ -d "$artifact_dir" ]; then
            cp "$artifact_dir"/*.json benchmark_results/releases/ 2>/dev/null || true
            cp "$artifact_dir"/*.md benchmark_results/releases/ 2>/dev/null || true
          fi
        done

        # Create an index of all results
        echo "# Benchmark Results Index" > benchmark_results/releases/INDEX.md
        echo "" >> benchmark_results/releases/INDEX.md
        echo "## Available Results" >> benchmark_results/releases/INDEX.md
        echo "" >> benchmark_results/releases/INDEX.md
        ls benchmark_results/releases/*.json 2>/dev/null | while read f; do
          basename "$f" >> benchmark_results/releases/INDEX.md
        done

    - name: Commit and push results
      run: |
        git config user.name "github-actions[bot]"
        git config user.email "github-actions[bot]@users.noreply.github.com"

        git add benchmark_results/releases/

        # Check if there are changes to commit
        if git diff --staged --quiet; then
          echo "No changes to commit"
          exit 0
        fi

        git commit -m "chore: Store benchmark results for ${{ github.ref_name }}"

        # Retry push with rebase to handle concurrent pushes
        MAX_RETRIES=3
        for i in $(seq 1 $MAX_RETRIES); do
          if git push; then
            echo "Successfully pushed results"
            exit 0
          fi
          echo "Push failed, attempt $i of $MAX_RETRIES. Rebasing and retrying..."
          git pull --rebase origin main
        done

        echo "Failed to push after $MAX_RETRIES attempts"
        exit 1
